#!/usr/bin/env python3
"""
M√≥dulo de Scraping Web Auxiliar
Realiza scraping de p√°ginas web com suporte a autentica√ß√£o para descoberta de:
- URLs e estrutura do site
- Formul√°rios e campos de entrada
- Endpoints de API
- Tecnologias utilizadas

Nota: Este m√≥dulo √© auxiliar e n√£o realiza testes de vulnerabilidades.
Para testes de seguran√ßa, use scanner_web_avancado.py
"""

import requests
import urllib.parse
import re
import time
import json
import base64
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils.logger import obter_logger


class VarreduraScraperAuth:
    """M√≥dulo de scraping web com autentica√ß√£o e descoberta avan√ßada"""

    def __init__(self):
        self.logger = obter_logger("ScraperAuth")
        self.session = requests.Session()

        # Configura√ß√µes
        self.timeout = 10
        self.max_pages = 200
        self.max_depth = 4
        self.max_workers = 8

        # Resultados
        self.urls_descobertas = set()
        self.formularios = []
        self.endpoints_api = []
        self.parametros_encontrados = set()
        self.tecnologias = {}
        self.estrutura_site = {}

                # Configura√ß√£o da sess√£o
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Cache-Control': 'max-age=0'
        })

        # Estado da autentica√ß√£o
        self.autenticado = False
        self.tokens_auth = {}
        self.cookies_auth = {}

    def executar(self, alvo: str, credenciais: Optional[Dict] = None,
                tipo_scan: str = "completo") -> Dict[str, any]:
        """
        Executa scraping com autentica√ß√£o

        Args:
            alvo: URL base ou dom√≠nio
            credenciais: Dict com 'usuario' e 'senha' para login
            tipo_scan: 'basico', 'autenticado', 'completo'

        Returns:
            Dict com resultados do scraping
        """
        self.logger.info(f"üï∑Ô∏è Iniciando scraping para: {alvo}")

        inicio = time.time()

        try:
            # Normalizar URL
            url_base = self._normalizar_url(alvo)

            # Inicializar resultados
            self._reset_resultados()

            # Fase 1: Spider b√°sico (sempre executado)
            self.logger.info("üîç Fase 1: Spider b√°sico...")
            self._spider_basico(url_base)

            # Fase 2: Detec√ß√£o de tecnologias
            self.logger.info("üîß Fase 2: Detectando tecnologias...")
            self._detectar_tecnologias(url_base)

            # Fase 3: An√°lise de formul√°rios
            self.logger.info("üìù Fase 3: Analisando formul√°rios...")
            self._analisar_formularios()

            # Fase 4: Descoberta de APIs
            self.logger.info("üîó Fase 4: Descobrindo APIs...")
            self._descobrir_apis()

            # Fase 5: Autentica√ß√£o (se credenciais fornecidas)
            if credenciais and tipo_scan in ['autenticado', 'completo']:
                self.logger.info(f"üîê Fase 5: Tentando autentica√ß√£o...")
                self.logger.info(f"   Credenciais: {bool(credenciais)}")
                self.logger.info(f"   Tipo scan: {tipo_scan}")
                sucesso_auth = self._tentar_autenticacao(url_base, credenciais)

                if sucesso_auth:
                    self.logger.info("‚úÖ Autentica√ß√£o bem-sucedida!")
                    # Spider autenticado
                    self.logger.info("üîç Fase 6: Spider autenticado...")
                    self._spider_autenticado(url_base)
                else:
                    self.logger.warning("‚ùå Falha na autentica√ß√£o")
            else:
                self.logger.info(f"üîê Autentica√ß√£o pulada - Credenciais: {bool(credenciais)}, Tipo: {tipo_scan}")

            # Fase 6: An√°lise final
            self.logger.info("üìä Fase 7: An√°lise final...")
            analise_final = self._analise_final()

            duracao = time.time() - inicio

            resultado = {
                'url_base': url_base,
                'tipo_scan': tipo_scan,
                'timestamp': datetime.now().isoformat(),
                'duracao_segundos': round(duracao, 2),
                'autenticacao': credenciais is not None,
                'urls_descobertas': list(self.urls_descobertas),
                'total_urls': len(self.urls_descobertas),
                'formularios': self.formularios,
                'total_formularios': len(self.formularios),
                'endpoints_api': self.endpoints_api,
                'total_apis': len(self.endpoints_api),
                'parametros_encontrados': list(self.parametros_encontrados),
                'total_parametros': len(self.parametros_encontrados),
                'tecnologias': self.tecnologias,
                'estrutura_site': self.estrutura_site,
                'vulnerabilidades': self.vulnerabilidades,
                'total_endpoints': len(self.endpoints_api),
                'analise_final': analise_final
            }

            self.logger.info(f"‚úÖ Scraping conclu√≠do: {len(self.urls_descobertas)} URLs encontradas")
            return resultado

        except Exception as e:
            self.logger.error(f"‚ùå Erro no scraping: {e}")
            return {'erro': str(e), 'url_base': alvo}

    def _normalizar_url(self, alvo: str) -> str:
        """Normaliza URL para formato padr√£o"""
        if not alvo.startswith(('http://', 'https://')):
            alvo = f"https://{alvo}"

        # Remover barra final se existir
        return alvo.rstrip('/')

    def _reset_resultados(self):
        """Reseta resultados para nova execu√ß√£o"""
        self.urls_descobertas = set()
        self.formularios = []
        self.endpoints_api = []
        self.parametros_encontrados = set()
        self.tecnologias = {}
        self.estrutura_site = {}

    def _spider_basico(self, url_base: str):
        """Spider b√°sico sem autentica√ß√£o"""
        urls_para_visitar = {url_base}
        urls_visitadas = set()
        depth = 0

        while urls_para_visitar and depth < self.max_depth and len(self.urls_descobertas) < self.max_pages:
            proximas_urls = set()

            for url in list(urls_para_visitar):
                if url in urls_visitadas:
                    continue

                try:
                    resp = self.session.get(url, timeout=self.timeout, verify=False,
                                          allow_redirects=True)

                    if resp.status_code == 200:
                        urls_visitadas.add(url)
                        self.urls_descobertas.add(url)

                        # Extrair links
                        links = self._extrair_links(resp.text, url)
                        for link in links:
                            if self._is_same_domain(link, url_base) and link not in urls_visitadas:
                                proximas_urls.add(link)

                        # Extrair formul√°rios
                        forms = self._extrair_formularios(resp.text, url)
                        self.formularios.extend(forms)

                        # Extrair par√¢metros de URLs
                        self._extrair_parametros_url(url)

                except Exception as e:
                    self.logger.debug(f"Erro ao visitar {url}: {e}")

            urls_para_visitar = proximas_urls - urls_visitadas
            depth += 1

    def _extrair_links(self, html: str, base_url: str) -> Set[str]:
        """Extrai links do HTML"""
        links = set()

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Links de √¢ncoras
            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urllib.parse.urljoin(base_url, href)
                if self._is_valid_url(full_url):
                    links.add(full_url)

            # Links de formul√°rios
            for form in soup.find_all('form', action=True):
                action = form['action']
                full_url = urllib.parse.urljoin(base_url, action)
                if self._is_valid_url(full_url):
                    links.add(full_url)

            # Scripts externos
            for script in soup.find_all('script', src=True):
                src = script['src']
                if not src.startswith(('http://', 'https://', '//')):
                    continue
                full_url = urllib.parse.urljoin(base_url, src)
                if self._is_valid_url(full_url):
                    links.add(full_url)

            # CSS externos
            for link in soup.find_all('link', href=True):
                if link.get('rel', [''])[0] == 'stylesheet':
                    href = link['href']
                    full_url = urllib.parse.urljoin(base_url, href)
                    if self._is_valid_url(full_url):
                        links.add(full_url)

        except Exception as e:
            self.logger.debug(f"Erro ao extrair links: {e}")

        return links

    def _extrair_formularios(self, html: str, url: str) -> List[Dict]:
        """Extrai formul√°rios do HTML"""
        formularios = []

        try:
            soup = BeautifulSoup(html, 'html.parser')

            for form in soup.find_all('form'):
                form_data = {
                    'url': url,
                    'action': form.get('action', ''),
                    'method': form.get('method', 'GET').upper(),
                    'inputs': []
                }

                # Extrair inputs
                for input_tag in form.find_all(['input', 'textarea', 'select']):
                    input_info = {
                        'name': input_tag.get('name', ''),
                        'type': input_tag.get('type', 'text'),
                        'value': input_tag.get('value', ''),
                        'required': input_tag.has_attr('required')
                    }
                    form_data['inputs'].append(input_info)

                if form_data['inputs']:
                    formularios.append(form_data)

        except Exception as e:
            self.logger.debug(f"Erro ao extrair formul√°rios: {e}")

        return formularios

    def _extrair_parametros_url(self, url: str):
        """Extrai par√¢metros de URL"""
        try:
            parsed = urllib.parse.urlparse(url)
            if parsed.query:
                params = urllib.parse.parse_qs(parsed.query)
                for param in params.keys():
                    self.parametros_encontrados.add(param)
        except Exception as e:
            self.logger.debug(f"Erro ao extrair par√¢metros: {e}")

    def _is_same_domain(self, url1: str, url2: str) -> bool:
        """Verifica se URLs s√£o do mesmo dom√≠nio"""
        try:
            parsed1 = urllib.parse.urlparse(url1)
            parsed2 = urllib.parse.urlparse(url2)
            return parsed1.netloc == parsed2.netloc
        except:
            return False

    def _is_valid_url(self, url: str) -> bool:
        """Verifica se URL √© v√°lida e n√£o √© externa"""
        try:
            parsed = urllib.parse.urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False

    def _detectar_tecnologias(self, url_base: str):
        """Detecta tecnologias usadas no site"""
        try:
            resp = self.session.get(url_base, timeout=self.timeout, verify=False)

            # Detectar por headers
            server = resp.headers.get('Server', '')
            if server:
                if 'apache' in server.lower():
                    self.tecnologias['web_server'] = 'Apache'
                elif 'nginx' in server.lower():
                    self.tecnologias['web_server'] = 'Nginx'
                elif 'iis' in server.lower():
                    self.tecnologias['web_server'] = 'IIS'

            # Detectar por conte√∫do
            content = resp.text.lower()

            # CMS
            if 'wordpress' in content:
                self.tecnologias['cms'] = 'WordPress'
            elif 'joomla' in content:
                self.tecnologias['cms'] = 'Joomla'
            elif 'drupal' in content:
                self.tecnologias['cms'] = 'Drupal'

            # Frameworks
            if 'laravel' in content:
                self.tecnologias['framework'] = 'Laravel'
            elif 'django' in content:
                self.tecnologias['framework'] = 'Django'
            elif 'react' in content:
                self.tecnologias['frontend'] = 'React'
            elif 'vue' in content:
                self.tecnologias['frontend'] = 'Vue.js'

            # Linguagens
            if '.php' in url_base or 'php' in content:
                self.tecnologias['language'] = 'PHP'
            elif '.jsp' in url_base:
                self.tecnologias['language'] = 'Java'
            elif '.asp' in url_base:
                self.tecnologias['language'] = 'ASP.NET'

        except Exception as e:
            self.logger.debug(f"Erro ao detectar tecnologias: {e}")

    def _analisar_formularios(self):
        """Analisa formul√°rios encontrados"""
        for form in self.formularios:
            # Verificar se √© formul√°rio de login
            is_login = any(
                input_field['name'].lower() in ['password', 'pass', 'pwd', 'senha', 'login']
                for input_field in form['inputs']
            )

            if is_login:
                form['tipo'] = 'login'
                # Verificar prote√ß√µes
                self._verificar_protecoes_login(form)
            else:
                form['tipo'] = 'generico'
                # Verificar outras vulnerabilidades
                self._verificar_protecoes_geral(form)

    def _verificar_protecoes_login(self, form: Dict):
        """Verifica prote√ß√µes em formul√°rio de login"""
        # Verificar HTTPS
        if not form['url'].startswith('https://'):
            self._adicionar_vulnerabilidade(
                'Login sem HTTPS',
                f'Formul√°rio de login em {form["url"]} n√£o usa HTTPS',
                'ALTA',
                form['url']
            )

        # Verificar CSRF token
        has_csrf = any(
            input_field['name'].lower() in ['csrf', 'token', '_token', 'authenticity_token']
            for input_field in form['inputs']
        )

        if not has_csrf:
            self._adicionar_vulnerabilidade(
                'Login sem prote√ß√£o CSRF',
                f'Formul√°rio de login em {form["url"]} n√£o possui token CSRF',
                'M√âDIA',
                form['url']
            )

    def _verificar_protecoes_geral(self, form: Dict):
        """Verifica prote√ß√µes gerais em formul√°rios"""
        # Verificar campos sem valida√ß√£o aparente
        for input_field in form['inputs']:
            if input_field['type'] in ['text', 'email', 'url'] and not input_field['required']:
                # Campo opcional - verificar se pode ser usado para inje√ß√£o
                pass

    def _descobrir_apis(self):
        """Descobre endpoints de API"""
        for url in self.urls_descobertas:
            try:
                resp = self.session.get(url, timeout=self.timeout, verify=False)

                # Procurar por endpoints em JavaScript
                js_endpoints = self._extrair_endpoints_js(resp.text, url)
                self.endpoints_api.extend(js_endpoints)

                # Procurar por padr√µes de API comuns
                api_patterns = [
                    r'/api/v\d+/[^\'"\s]+',
                    r'/rest/[^\'"\s]+',
                    r'/graphql',
                    r'/swagger',
                    r'/docs/api'
                ]

                for pattern in api_patterns:
                    matches = re.findall(pattern, resp.text)
                    for match in matches:
                        full_url = urllib.parse.urljoin(url, match)
                        if full_url not in [api['url'] for api in self.endpoints_api]:
                            self.endpoints_api.append({
                                'url': full_url,
                                'tipo': 'descoberto',
                                'fonte': url
                            })

            except Exception as e:
                self.logger.debug(f"Erro ao analisar {url}: {e}")

    def _extrair_endpoints_js(self, html: str, base_url: str) -> List[Dict]:
        """Extrai endpoints de API de c√≥digo JavaScript"""
        endpoints = []

        try:
            # Padr√µes comuns de endpoints em JS
            patterns = [
                r'["\'](/api/[^"\']+)["\']',
                r'["\'](https?://[^"\']+/api/[^"\']+)["\']',
                r'fetch\(["\']([^"\']+)["\']',
                r'axios\.(get|post|put|delete)\(["\']([^"\']+)["\']',
                r'\$\.(get|post|ajax)\(["\']([^"\']+)["\']'
            ]

            for pattern in patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        endpoint = match[1] if len(match) > 1 else match[0]
                    else:
                        endpoint = match

                    if endpoint.startswith('/'):
                        full_url = urllib.parse.urljoin(base_url, endpoint)
                    elif endpoint.startswith(('http://', 'https://')):
                        full_url = endpoint
                    else:
                        continue

                    if full_url not in [api['url'] for api in endpoints]:
                        endpoints.append({
                            'url': full_url,
                            'tipo': 'javascript',
                            'fonte': base_url
                        })

        except Exception as e:
            self.logger.debug(f"Erro ao extrair endpoints JS: {e}")

        return endpoints

    def _tentar_autenticacao(self, url_base: str, credenciais: Dict) -> bool:
        """Tenta fazer login no site com captura avan√ßada de tokens"""
        try:
            # Procurar formul√°rio de login
            login_form = None
            for form in self.formularios:
                if form.get('tipo') == 'login':
                    login_form = form
                    break

            if not login_form:
                self.logger.warning("Nenhum formul√°rio de login encontrado")
                return False

            self.logger.info(f"üìù Formul√°rio de login encontrado: {login_form['url']}")
            self.logger.info(f"   Action: {login_form['action']}")
            self.logger.info(f"   Method: {login_form['method']}")

            # Preparar dados do login
            login_data = {}
            for input_field in login_form['inputs']:
                name = input_field['name'].lower()
                if name in ['username', 'user', 'login', 'email']:
                    login_data[input_field['name']] = credenciais.get('usuario', '')
                    self.logger.info(f"   Campo usu√°rio: {input_field['name']} = {credenciais.get('usuario', '')}")
                elif name in ['password', 'pass', 'pwd', 'senha']:
                    login_data[input_field['name']] = credenciais.get('senha', '')
                    self.logger.info(f"   Campo senha: {input_field['name']} = {'*' * len(credenciais.get('senha', ''))}")
                elif name in ['captcha', 'ct_captcha'] and input_field.get('value'):
                    # Manter valor padr√£o do captcha se existir
                    login_data[input_field['name']] = input_field['value']
                    self.logger.info(f"   Campo captcha: {input_field['name']} = {input_field['value']}")
                elif input_field.get('value') and input_field['type'] != 'password':
                    # Incluir outros campos com valores padr√£o
                    login_data[input_field['name']] = input_field['value']
                    self.logger.info(f"   Campo adicional: {input_field['name']} = {input_field['value']}")

            self.logger.info(f"üìù Tentando login com usu√°rio: {credenciais.get('usuario', 'N/A')}")
            self.logger.info(f"   Dados do login: {list(login_data.keys())}")

            # Fazer login
            action_url = urllib.parse.urljoin(login_form['url'], login_form['action'])
            self.logger.info(f"   URL do login: {action_url}")

            resp = self.session.post(
                action_url,
                data=login_data,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )

            self.logger.info(f"   Status da resposta: {resp.status_code}")
            self.logger.info(f"   URL final: {resp.url}")
            self.logger.info(f"   Hist√≥rico de redirecionamentos: {len(resp.history)}")

            # Capturar tokens e cookies de autentica√ß√£o
            self._capturar_tokens_autenticacao(resp)

            # Verificar se login foi bem-sucedido
            sucesso = self._verificar_sucesso_login(resp, login_form['url'])

            if sucesso:
                self.autenticado = True
                self.logger.info("‚úÖ Login bem-sucedido!")
                self._log_tokens_capturados()
                return True
            else:
                self.logger.warning("‚ùå Falha no login")
                # Log detalhado do porqu√™ falhou
                self._debug_falha_login(resp, login_form['url'])
                return False

        except Exception as e:
            self.logger.error(f"Erro na autentica√ß√£o: {e}")
            return False

    def _capturar_tokens_autenticacao(self, response: requests.Response):
        """Captura tokens de autentica√ß√£o da resposta"""
        try:
            # Capturar Authorization header
            auth_header = response.headers.get('Authorization', '')
            if auth_header.startswith('Bearer '):
                self.tokens_auth['bearer'] = auth_header.replace('Bearer ', '')
                self.session.headers.update({'Authorization': auth_header})

            # Capturar cookies de sess√£o
            for cookie in self.session.cookies:
                if any(keyword in cookie.name.lower() for keyword in ['session', 'auth', 'token', 'jwt']):
                    self.cookies_auth[cookie.name] = cookie.value

            # Procurar tokens JWT no conte√∫do da resposta
            if response.text:
                # Padr√µes de JWT
                jwt_patterns = [
                    r'["\']token["\']\s*:\s*["\']([A-Za-z0-9-_]+\.[A-Za-z0-9-_]+\.[A-Za-z0-9-_]*)["\']',
                    r'["\']jwt["\']\s*:\s*["\']([A-Za-z0-9-_]+\.[A-Za-z0-9-_]+\.[A-Za-z0-9-_]*)["\']',
                    r'["\']access_token["\']\s*:\s*["\']([A-Za-z0-9-_]+\.[A-Za-z0-9-_]+\.[A-Za-z0-9-_]*)["\']'
                ]

                for pattern in jwt_patterns:
                    matches = re.findall(pattern, response.text)
                    for match in matches:
                        if self._validar_jwt(match):
                            self.tokens_auth['jwt'] = match
                            self.session.headers.update({
                                'Authorization': f'Bearer {match}'
                            })
                            break

            # Procurar tokens em localStorage/sessionStorage simulado
            storage_patterns = [
                r'localStorage\.setItem\(["\']([^"\']+)["\'],\s*["\']([^"\']+)["\']',
                r'sessionStorage\.setItem\(["\']([^"\']+)["\'],\s*["\']([^"\']+)["\']'
            ]

            for pattern in storage_patterns:
                matches = re.findall(pattern, response.text, re.IGNORECASE)
                for key, value in matches:
                    if 'token' in key.lower():
                        self.tokens_auth[f'storage_{key}'] = value

        except Exception as e:
            self.logger.debug(f"Erro ao capturar tokens: {e}")

    def _validar_jwt(self, token: str) -> bool:
        """Valida se √© um token JWT v√°lido"""
        try:
            parts = token.split('.')
            return len(parts) == 3 and all(parts)
        except:
            return False

    def _verificar_sucesso_login(self, response: requests.Response, login_url: str) -> bool:
        """Verifica se o login foi bem-sucedido"""
        try:
            # Verificar c√≥digo de status
            if response.status_code not in [200, 302, 301]:
                return False

            # Verificar se n√£o voltou para p√°gina de login
            current_url = response.url.lower()
            if any(keyword in current_url for keyword in ['login', 'auth', 'signin']):
                return False

            # Verificar presen√ßa de elementos de dashboard/menu
            content = response.text.lower()
            dashboard_indicators = [
                'dashboard', 'menu', 'logout', 'sair', 'perfil', 'profile',
                'admin', 'painel', 'sistema', 'home', 'inicio'
            ]

            if any(indicator in content for indicator in dashboard_indicators):
                return True

            # Verificar se tem cookies de sess√£o
            session_cookies = [c for c in self.session.cookies if 'session' in c.name.lower()]
            if session_cookies:
                return True

            # Verificar se tem tokens de auth
            if self.tokens_auth:
                return True

            # Verificar redirecionamento para √°rea protegida
            if len(response.history) > 0:
                final_url = response.url
                if final_url != login_url and 'login' not in final_url.lower():
                    return True

            return False

        except Exception as e:
            self.logger.debug(f"Erro ao verificar sucesso do login: {e}")
            return False

    def _log_tokens_capturados(self):
        """Log dos tokens capturados para debug"""
        if self.tokens_auth:
            self.logger.info("üîë Tokens de autentica√ß√£o capturados:")
            for tipo, token in self.tokens_auth.items():
                if 'bearer' in tipo.lower() or 'jwt' in tipo.lower():
                    # M√°scara o token para log
                    masked = token[:10] + "..." + token[-5:] if len(token) > 15 else token
                    self.logger.info(f"   {tipo}: {masked}")
                else:
                    self.logger.info(f"   {tipo}: {token}")

        if self.cookies_auth:
            self.logger.info("üç™ Cookies de autentica√ß√£o capturados:")
            for name, value in self.cookies_auth.items():
                masked_value = value[:5] + "..." if len(value) > 5 else value
                self.logger.info(f"   {name}: {masked_value}")

    def _spider_autenticado(self, url_base: str):
        """Spider avan√ßado com sess√£o autenticada"""
        if not self.autenticado:
            self.logger.warning("Spider autenticado chamado sem autentica√ß√£o")
            return

        self.logger.info("üîç Iniciando spider autenticado...")

        # URLs para explorar ap√≥s login
        urls_autenticadas = set()

        # Adicionar URLs comuns de sistemas web
        base_parsed = urllib.parse.urlparse(url_base)
        caminhos_comuns = [
            '/admin', '/dashboard', '/painel', '/sistema', '/home',
            '/usuario', '/profile', '/config', '/settings',
            '/api', '/rest', '/graphql', '/swagger',
            '/extension', '/desktop', '/menu', '/principal'
        ]

        for caminho in caminhos_comuns:
            url_teste = f"{base_parsed.scheme}://{base_parsed.netloc}{caminho}"
            urls_autenticadas.add(url_teste)

        # Adicionar URLs descobertas que parecem ser protegidas
        for url in self.urls_descobertas.copy():
            url_lower = url.lower()
            if any(keyword in url_lower for keyword in [
                'admin', 'dashboard', 'panel', 'user', 'profile',
                'config', 'settings', 'system', 'api', 'extension'
            ]):
                urls_autenticadas.add(url)

        # Explorar URLs autenticadas
        urls_visitadas = set()

        for url in urls_autenticadas:
            if url in urls_visitadas:
                continue

            try:
                self.logger.debug(f"üåê Explorando: {url}")

                # Fazer requisi√ß√£o com headers de autentica√ß√£o
                resp = self.session.get(
                    url,
                    timeout=self.timeout,
                    verify=False,
                    allow_redirects=True
                )

                urls_visitadas.add(url)

                if resp.status_code == 200:
                    # Verificar se realmente estamos autenticados
                    if self._verificar_autenticacao_ativa(resp):
                        self.urls_descobertas.add(url)

                        # Extrair novos links da p√°gina autenticada
                        novos_links = self._extrair_links(resp.text, url)
                        for link in novos_links:
                            if (self._is_same_domain(link, url_base) and
                                link not in self.urls_descobertas and
                                link not in urls_visitadas):
                                urls_autenticadas.add(link)

                        # Extrair novos formul√°rios
                        novos_forms = self._extrair_formularios(resp.text, url)
                        for form in novos_forms:
                            if form not in self.formularios:
                                self.formularios.append(form)

                        # Procurar por APIs e endpoints
                        self._extrair_endpoints_pagina(resp.text, url)

                        # Capturar mais tokens se encontrados
                        self._capturar_tokens_autenticacao(resp)

                        self.logger.debug(f"‚úÖ Explorado com sucesso: {url}")
                    else:
                        self.logger.debug(f"‚ùå P√°gina n√£o acess√≠vel (n√£o autenticado): {url}")
                else:
                    self.logger.debug(f"‚ùå Status {resp.status_code}: {url}")

            except Exception as e:
                self.logger.debug(f"Erro ao explorar {url}: {e}")

        self.logger.info(f"üîç Spider autenticado conclu√≠do: {len(urls_visitadas)} URLs exploradas")

    def _verificar_autenticacao_ativa(self, response: requests.Response) -> bool:
        """Verifica se a autentica√ß√£o ainda est√° ativa"""
        try:
            content = response.text.lower()
            url = response.url.lower()

            # Verificar se foi redirecionado para login
            if any(keyword in url for keyword in ['login', 'auth', 'signin']):
                return False

            # Verificar presen√ßa de elementos de sistema autenticado
            auth_indicators = [
                'logout', 'sair', 'dashboard', 'menu', 'admin',
                'perfil', 'profile', 'config', 'settings',
                'sistema', 'painel', 'desktop', 'extension'
            ]

            if any(indicator in content for indicator in auth_indicators):
                return True

            # Verificar se tem cookies de sess√£o ativos
            session_cookies = [c for c in self.session.cookies if not c.is_expired()]
            if session_cookies:
                return True

            return False

        except Exception as e:
            self.logger.debug(f"Erro ao verificar autentica√ß√£o: {e}")
            return False

    def _extrair_endpoints_pagina(self, html: str, base_url: str):
        """Extrai endpoints de API da p√°gina"""
        try:
            # Padr√µes de endpoints em HTML/JS
            endpoint_patterns = [
                r'["\']/api/[^"\']+["\']',
                r'["\']/rest/[^"\']+["\']',
                r'["\']/graphql[^"\']*["\']',
                r'["\']/extension/[^"\']+["\']',
                r'["\']/desktop/[^"\']+["\']',
                r'fetch\(["\']([^"\']+)["\']',
                r'axios\.(get|post|put|delete)\(["\']([^"\']+)["\']',
                r'\$\.(get|post|ajax)\(["\']([^"\']+)["\']'
            ]

            for pattern in endpoint_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        endpoint = match[1] if len(match) > 1 else match[0]
                    else:
                        endpoint = match

                    if endpoint.startswith('/'):
                        full_url = urllib.parse.urljoin(base_url, endpoint)
                    elif endpoint.startswith(('http://', 'https://')):
                        full_url = endpoint
                    else:
                        continue

                    # Verificar se j√° existe
                    if not any(api['url'] == full_url for api in self.endpoints_api):
                        self.endpoints_api.append({
                            'url': full_url,
                            'tipo': 'descoberto_autenticado',
                            'fonte': base_url
                        })

        except Exception as e:
            self.logger.debug(f"Erro ao extrair endpoints: {e}")

    def _analise_final(self) -> Dict[str, any]:
        """Realiza an√°lise final dos resultados"""
        analise = {
            'total_urls': len(self.urls_descobertas),
            'total_formularios': len(self.formularios),
            'total_apis': len(self.endpoints_api),
            'total_parametros': len(self.parametros_encontrados),
            'total_vulnerabilidades': len(self.vulnerabilidades),
            'autenticacao': {
                'status': self.autenticado,
                'tokens_capturados': len(self.tokens_auth),
                'cookies_autenticacao': len(self.cookies_auth),
                'tipos_token': list(self.tokens_auth.keys())
            },
            'por_criticidade': {},
            'tipos_formulario': {},
            'tecnologias_principais': self.tecnologias
        }

        # Contar por criticidade
        for vuln in self.vulnerabilidades:
            crit = vuln.get('criticidade', 'BAIXA')
            analise['por_criticidade'][crit] = analise['por_criticidade'].get(crit, 0) + 1

        # Contar tipos de formul√°rio
        for form in self.formularios:
            tipo = form.get('tipo', 'generico')
            analise['tipos_formulario'][tipo] = analise['tipos_formulario'].get(tipo, 0) + 1

        return analise


# Fun√ß√µes de compatibilidade
def executar_scraper_web(alvo: str, credenciais: Optional[Dict] = None,
                        tipo_scan: str = "completo") -> Dict[str, any]:
    """Fun√ß√£o compat√≠vel com sistema existente"""
    scraper = VarreduraScraperAuth()
    return scraper.executar(alvo, credenciais, tipo_scan)


def scraper_web_basico(alvo: str) -> Dict[str, any]:
    """Spider b√°sico sem autentica√ß√£o"""
    return executar_scraper_web(alvo, tipo_scan="basico")


def scraper_web_autenticado(alvo: str, usuario: str, senha: str) -> Dict[str, any]:
    """Spider com autentica√ß√£o"""
    credenciais = {'usuario': usuario, 'senha': senha}
    return executar_scraper_web(alvo, credenciais, tipo_scan="autenticado")


def main():
    """Teste do m√≥dulo"""
    import sys

    if len(sys.argv) < 2:
        print("Uso: python varredura_scraper_auth.py <url> [usuario] [senha]")
        return

    url = sys.argv[1]
    credenciais = None

    if len(sys.argv) >= 4:
        credenciais = {
            'usuario': sys.argv[2],
            'senha': sys.argv[3]
        }

    scraper = VarreduraScraperAuth()
    resultado = scraper.executar(url, credenciais)

    print(json.dumps(resultado, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
