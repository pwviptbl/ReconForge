# WebCrawlerPlugin - Navega√ß√£o Web Avan√ßada com Selenium

O **WebCrawlerPlugin** √© um plugin avan√ßado para o VarreduraIA que utiliza Selenium para navega√ß√£o web automatizada, an√°lise de formul√°rios, tentativas de login autom√°tico e mapeamento completo de aplica√ß√µes web.

## üöÄ Funcionalidades Principais

### üìù An√°lise de Formul√°rios
- **Detec√ß√£o autom√°tica** de formul√°rios em p√°ginas web
- **Identifica√ß√£o de campos** (username, password, email, etc.)
- **Classifica√ß√£o autom√°tica** de formul√°rios de login
- **Extra√ß√£o de tokens CSRF** e campos ocultos
- **Mapeamento de par√¢metros** de entrada

### üîê Login Autom√°tico
- **Tentativas autom√°ticas** com credenciais comuns
- **Detec√ß√£o inteligente** de formul√°rios de login
- **Verifica√ß√£o de sucesso** baseada em indicadores de resposta
- **Suporte a redirecionamentos** p√≥s-login
- **An√°lise de respostas** para determinar falhas/sucessos

### üï∑Ô∏è Crawling Inteligente
- **Navega√ß√£o em profundidade** configur√°vel
- **Extra√ß√£o de links** e mapeamento de estrutura
- **Suporte a JavaScript** e SPAs
- **An√°lise de cookies** e sess√µes
- **Screenshots autom√°ticos** em caso de erro

### üõ†Ô∏è Detec√ß√£o de Tecnologias
- **Identifica√ß√£o autom√°tica** de frameworks (WordPress, Laravel, React, etc.)
- **An√°lise de JavaScript** e bibliotecas
- **Detec√ß√£o de padr√µes** no c√≥digo-fonte
- **Mapeamento de APIs** e endpoints

### üîí An√°lise de Seguran√ßa
- **Verifica√ß√£o de headers** de seguran√ßa
- **An√°lise de cookies** (HttpOnly, Secure, SameSite)
- **Detec√ß√£o de vulnerabilidades** comuns
- **Mapeamento de superf√≠cie de ataque**

## üìã Configura√ß√µes Dispon√≠veis

```yaml
WebCrawlerPlugin:
  headless: true                    # Executar sem interface gr√°fica
  timeout: 30                       # Timeout para opera√ß√µes
  page_load_timeout: 60            # Timeout para carregamento de p√°ginas
  implicit_wait: 10                # Espera impl√≠cita do Selenium
  max_depth: 3                     # Profundidade m√°xima de crawling
  max_pages: 50                    # N√∫mero m√°ximo de p√°ginas a navegar
  screenshot_on_error: true        # Tirar screenshot em caso de erro
  follow_redirects: true           # Seguir redirecionamentos
  analyze_forms: true              # Analisar formul√°rios encontrados
  attempt_login: true              # Tentar login autom√°tico
  common_credentials: true         # Usar credenciais comuns
  javascript_enabled: true         # Habilitar JavaScript
  user_agent: "Mozilla/5.0..."     # User-Agent para requisi√ß√µes
  window_size: [1920, 1080]       # Tamanho da janela do browser
  extract_apis: true               # Extrair endpoints de API
  analyze_cookies: true            # Analisar cookies
  check_security_headers: true     # Verificar headers de seguran√ßa
  detect_frameworks: true          # Detectar frameworks/tecnologias
```

## üéØ Uso B√°sico

### Via Sistema Principal
```bash
# Executar varredura completa
python main.py https://exemplo.com

# O WebCrawlerPlugin ser√° executado automaticamente se habilitado
```

### Via Plugin Manager
```bash
# Verificar status do plugin
python manage_plugins.py list

# Ver configura√ß√£o atual
python manage_plugins.py config WebCrawlerPlugin

# Habilitar/desabilitar
python manage_plugins.py enable WebCrawlerPlugin
python manage_plugins.py disable WebCrawlerPlugin
```

### Via C√≥digo Python
```python
from plugins.web_crawler_plugin import WebCrawlerPlugin

# Criar inst√¢ncia do plugin
plugin = WebCrawlerPlugin()

# Configurar se necess√°rio
plugin.config.update({
    'max_pages': 10,
    'attempt_login': True,
    'headless': False  # Mostrar browser para debug
})

# Executar
result = plugin.execute(
    target='https://exemplo.com',
    context={'test_mode': True}
)

# Verificar resultados
if result.success:
    data = result.data['web_crawling']
    print(f"P√°ginas navegadas: {data['statistics']['total_pages']}")
    print(f"Formul√°rios encontrados: {data['statistics']['total_forms']}")
    print(f"Tentativas de login: {data['statistics']['login_attempts']}")
```

## üìä Resultados Produzidos

### Estrutura de Dados
```json
{
  "web_crawling": {
    "target": "https://exemplo.com",
    "timestamp": 1234567890,
    "pages_crawled": [
      {
        "url": "https://exemplo.com",
        "title": "T√≠tulo da P√°gina",
        "depth": 0,
        "forms": [...],
        "links": [...],
        "inputs": [...],
        "cookies": [...],
        "technologies": [...]
      }
    ],
    "forms_found": [
      {
        "url": "https://exemplo.com/login",
        "method": "post",
        "action": "/authenticate",
        "is_login_form": true,
        "inputs": [...],
        "csrf_tokens": [...]
      }
    ],
    "login_attempts": [
      {
        "username": "admin",
        "password": "admin",
        "success": false,
        "final_url": "...",
        "url_changed": false
      }
    ],
    "frameworks_detected": ["WordPress", "jQuery", "Bootstrap"],
    "parameters_discovered": {
      "get_params": ["page", "id", "search"],
      "form_params": ["username", "password", "_token"],
      "cookie_names": ["PHPSESSID", "_session"]
    },
    "security_headers": {
      "headers_found": {"X-Frame-Options": "SAMEORIGIN"},
      "missing_headers": ["Content-Security-Policy"],
      "security_score": 0.375
    },
    "statistics": {
      "total_pages": 5,
      "total_forms": 2,
      "total_parameters": 15,
      "login_attempts": 3,
      "frameworks_detected": 3
    }
  }
}
```

## üß™ Testes Dispon√≠veis

### Teste B√°sico (Configura√ß√£o)
```bash
python test_web_crawler_unit.py
```
- Verifica imports e configura√ß√£o b√°sica
- Testa valida√ß√£o de URLs
- Verifica detec√ß√£o de frameworks
- Testa an√°lise de formul√°rios

### Teste Simples (Selenium)
```bash
python test_simple_crawler.py
```
- Navega√ß√£o b√°sica com Selenium
- Teste com site simples (httpbin.org)
- Verifica√ß√£o de funcionalidades principais

### Teste Avan√ßado (Formul√°rios)
```bash
python test_advanced_crawler.py
```
- An√°lise completa de formul√°rios
- Tentativas de login autom√°tico
- Teste com sites reais
- An√°lise detalhada de resultados

## üîß Depend√™ncias

### Principais
- **selenium** (4.35.0+) - Automa√ß√£o do browser
- **webdriver-manager** - Gerenciamento autom√°tico do ChromeDriver

### Opcionais (mas recomendadas)
- **requests** - Para verifica√ß√µes HTTP adicionais
- **beautifulsoup4** - Parsing HTML adicional
- **lxml** - Parser XML/HTML r√°pido

## üì± Requisitos do Sistema

### Browser
- **Google Chrome** ou **Chromium** instalado
- Vers√£o recente (√∫ltimos 2 anos)

### Sistema Operacional
- **Linux** (testado no Ubuntu/Debian)
- **Windows** (com Chrome instalado)
- **macOS** (com Chrome instalado)

## ‚ö†Ô∏è Considera√ß√µes Importantes

### Performance
- O plugin pode ser **lento** para sites grandes
- **Configure max_pages e max_depth** adequadamente
- Use **headless=true** para melhor performance

### Seguran√ßa
- **N√£o use em produ√ß√£o** com attempt_login=true
- As **credenciais testadas s√£o comuns** e p√∫blicas
- **Respeite robots.txt** e termos de uso dos sites

### Rate Limiting
- O plugin **n√£o implementa rate limiting autom√°tico**
- Para sites sens√≠veis, adicione delays manuais
- **Configure timeouts apropriados** para evitar bloqueios

## üéÅ Exemplos de Uso

### 1. An√°lise de Formul√°rios Espec√≠fica
```python
plugin = WebCrawlerPlugin()
plugin.config.update({
    'analyze_forms': True,
    'attempt_login': False,  # S√≥ analisar, n√£o tentar login
    'max_pages': 5
})
```

### 2. Teste de Login Controlado
```python
plugin.config.update({
    'attempt_login': True,
    'common_credentials': True,
    'screenshot_on_error': True,  # Debug visual
    'headless': False  # Ver o que est√° acontecendo
})
```

### 3. Mapeamento Completo de Site
```python
plugin.config.update({
    'max_depth': 5,
    'max_pages': 100,
    'extract_apis': True,
    'detect_frameworks': True,
    'analyze_cookies': True
})
```

## üÜò Troubleshooting

### Chrome n√£o encontrado
```bash
# Ubuntu/Debian
sudo apt update && sudo apt install google-chrome-stable

# Ou usar Chromium
sudo apt install chromium-browser
```

### Timeout errors
- Aumente `page_load_timeout` e `timeout`
- Verifique conectividade de rede
- Use sites mais simples para teste

### Selenium errors
- Atualize Chrome para vers√£o mais recente
- Reinstale webdriver-manager: `pip install --upgrade webdriver-manager`
- Verifique se tem permiss√µes para criar arquivos tempor√°rios

---

**üí° Dica:** Para desenvolvimento e debug, configure `headless=false` para ver o browser em a√ß√£o!
